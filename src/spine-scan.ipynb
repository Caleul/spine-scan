{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eb3e39-098e-483c-9bb2-d5127a6bf8d7",
   "metadata": {},
   "source": [
    "# Instalação e importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ec8da3-3c51-4afc-99cd-949940be1d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless numpy tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1506d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d1588-4b4f-48d2-a603-dc813b703855",
   "metadata": {},
   "source": [
    "**Atenção:** O aviso acima apenas informa que não foram encontrados drivers cuda e por conta disso será usada a CPU da máquina ao invés da GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28962c8a",
   "metadata": {},
   "source": [
    "# Coleta e processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1ed72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    img_normalized = img_resized / 255.0\n",
    "    return img_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6807d",
   "metadata": {},
   "source": [
    "## Balanceamento do DataSet\n",
    "Para evitar maior peso em um determinado tipo de dado, foi feito um balanceamento do dataset.\n",
    "\n",
    "Para isso, foi utilizado o método de reamostragem chamado de undersampling, no qual foi selecionado aleatoriamente os dados de uma classe e duplicado até que a quantidade de dados de cada classe se iguale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ad1f84-059e-439a-9177-d6eaa951629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset balanceado criado em: ./balanced-dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = './dataset'\n",
    "balanced_dataset_dir = './balanced-dataset'\n",
    "\n",
    "classes = ['healthy', 'kyphosis', 'lordosis', 'kyphosis_and_lordosis']\n",
    "\n",
    "os.makedirs(balanced_dataset_dir, exist_ok=True)\n",
    "\n",
    "image_counts = {}\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(dataset_dir, cls)\n",
    "    image_counts[cls] = len(os.listdir(cls_path))\n",
    "\n",
    "max_images = max(image_counts.values())\n",
    "new_image_counts = {}\n",
    "\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(balanced_dataset_dir, cls), exist_ok=True)\n",
    "    \n",
    "    images = os.listdir(os.path.join(dataset_dir, cls))\n",
    "    \n",
    "    for index, image in enumerate(images):\n",
    "        new_name = f\"{cls}_{index + 1:03d}.jpg\"\n",
    "        shutil.copy(os.path.join(dataset_dir, cls, image), \n",
    "                    os.path.join(balanced_dataset_dir, cls, new_name))\n",
    "    \n",
    "    new_image_counts[cls] = len(images)\n",
    "\n",
    "    current_count = new_image_counts[cls]\n",
    "    while current_count < max_images:\n",
    "        image_to_copy = random.choice(images)\n",
    "        \n",
    "        new_name = f\"{cls}_{current_count + 1:03d}.jpg\"\n",
    "        shutil.copy(os.path.join(dataset_dir, cls, image_to_copy),\n",
    "                    os.path.join(balanced_dataset_dir, cls, new_name))\n",
    "        \n",
    "        current_count += 1\n",
    "\n",
    "print(\"Dataset balanceado criado em:\", balanced_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e21c3d",
   "metadata": {},
   "source": [
    "### Criando CNN (rede neural convolucional)\n",
    "Essa função cria um modelo CNN que passa por algumas camadas convolucionais para extrair características das imagens, nas quais foram utilizados as funções:\n",
    "- ReLU \n",
    "\t- Função de ativação que permite que a rede neural aprenda padrões complexos\n",
    "- Pooling\n",
    "\t- Operação de amostragem usada em redes convolucionais para reduzir as dimensões e/ou tamanho da imagem ou das saídas das camadas convolucionais, porém, preservando suas características mais importantes\n",
    "- Normalização de Batch\n",
    "\t- Normalização de dados de entrada para que a rede neural possa aprender mais rapidamente\n",
    "- Softmax\n",
    "\t- Função de ativação que converte um vetor de valores reais em uma distribuição de probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5430f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multilabel_cnn(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(3, (1, 1), padding=\"same\")(inputs)\n",
    "    \n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(x)\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    outputs = layers.Dense(3, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_2_class_cnn(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(3, (1, 1), padding=\"same\")(inputs)\n",
    "    \n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(x)\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (224, 224, 1)\n",
    "\n",
    "generalist_cnn_model = create_multilabel_cnn(input_shape)\n",
    "lordosis_cnn_model = create_2_class_cnn(input_shape)\n",
    "kiphosis_cnn_model = create_2_class_cnn(input_shape)\n",
    "\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439ed3e",
   "metadata": {},
   "source": [
    "## Gerador de alterações nas imagens\n",
    "\n",
    "Para melhorar a amplitude do treinamento, foi utilizado o gerador de alterações nas imagens, que trás alterações em rotação, deslocamento, zoom, brilho e direção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ce7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    brightness_range = [0.8, 1.2],\n",
    "    validation_split = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c67cd",
   "metadata": {},
   "source": [
    "# Classificação multilabel\n",
    "Usado no modelo generalista para classificar mais de uma classe em um único caso.\n",
    "Apontando a classe correta para cada caso.\n",
    "O único caso que difere dos demais modelos é o 'kyphosis_and_lordosis', para isso, foi criado a classificação dessa classe específica para ser tanto o caso de cifose, quanto o caso de lordose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c5b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilabel_labels(directory):\n",
    "    labels = []\n",
    "    for class_name in ['healthy', 'kyphosis', 'lordosis', 'kyphosis_and_lordosis']:\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            for img in os.listdir(class_dir):\n",
    "                if class_name == 'kyphosis_and_lordosis':\n",
    "                    labels.append([0, 1, 1])\n",
    "                elif class_name == 'healthy':\n",
    "                    labels.append([1, 0, 0])\n",
    "                elif class_name == 'kyphosis':\n",
    "                    labels.append([0, 1, 0])\n",
    "                elif class_name == 'lordosis':\n",
    "                    labels.append([0, 0, 1])\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885e959",
   "metadata": {},
   "source": [
    "# Pré-processamento dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767f6f0",
   "metadata": {},
   "source": [
    "Especificamente no caso generalista, as classes foram definidas previamente, por isso a função `multilabel_data_generator` foi criada, para fazer corretamente o apontamento da classe 'kyphosis_and_lordosis' para ambas as classes 'kyphosis' e 'lordosis'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3832254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232 images belonging to 4 classes.\n",
      "Found 96 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "generalist_classes = ['healthy', 'kyphosis', 'lordosis']\n",
    "\n",
    "def multilabel_data_generator(generator, labels):\n",
    "    while True:\n",
    "        data = next(generator)\n",
    "        batch_size = data.shape[0]\n",
    "\n",
    "        yield data, labels[:batch_size]\n",
    "            \n",
    "initial_generalist_train_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = None,\n",
    "    shuffle = True,\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "initial_generalist_validation_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = None,\n",
    "    shuffle = True,\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "labels = get_multilabel_labels('./balanced-dataset')\n",
    "\n",
    "generalist_train_generator = multilabel_data_generator(initial_generalist_train_generator, labels)\n",
    "generalist_validation_generator = multilabel_data_generator(initial_generalist_validation_generator, labels)\n",
    "\n",
    "generalist_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a7d18",
   "metadata": {},
   "source": [
    "Para aprimorar o treinamento do modelo, foi criada as funções `kyphosis_custom_generator` e `lordosis_custom_generator`, que tem como objetivo considerar as imagens da pasta `kyphosis_and_lordosis` para os casos de cifose, no modelo de cifose, e para os casos de lordose, no modelo de lordose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8d60c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kyphosis_custom_generator(data_dir, kiphosis_classes, target_size=(224, 224), batch_size=4, subset='training'):\n",
    "    # Mapeamento dos diretórios\n",
    "    healthy_dir = os.path.join(data_dir, 'healthy')\n",
    "    kyphosis_dir = os.path.join(data_dir, 'kyphosis')\n",
    "    kyphosis_and_lordosis_dir = os.path.join(data_dir, 'kyphosis_and_lordosis')\n",
    "    \n",
    "    # Carregar caminhos de imagens\n",
    "    healthy_images = [os.path.join(healthy_dir, img) for img in os.listdir(healthy_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    kyphosis_images = [os.path.join(kyphosis_dir, img) for img in os.listdir(kyphosis_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    kyphosis_and_lordosis_images = [os.path.join(kyphosis_and_lordosis_dir, img) for img in os.listdir(kyphosis_and_lordosis_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    \n",
    "    # Associação das imagens com as classes\n",
    "    images = healthy_images + kyphosis_images + kyphosis_and_lordosis_images\n",
    "    labels = [0] * len(healthy_images) + [1] * (len(kyphosis_images) + len(kyphosis_and_lordosis_images))\n",
    "    labels = to_categorical(labels, num_classes=2)\n",
    "\n",
    "    # Divisão entre treinamento e validação\n",
    "    split_index = int(len(images) * 0.8)\n",
    "    if subset == 'training':\n",
    "        images, labels = images[:split_index], labels[:split_index]\n",
    "    else:\n",
    "        images, labels = images[split_index:], labels[split_index:]\n",
    "\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Embaralhar as imagens e labels\n",
    "        indices = np.arange(len(images))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in indices:\n",
    "            img = load_img(images[i], target_size=target_size, color_mode='grayscale')  # Alterado para RGB\n",
    "            img_array = img_to_array(img)\n",
    "            batch_images.append(img_array)\n",
    "            batch_labels.append(labels[i])\n",
    "\n",
    "            if len(batch_images) == batch_size:\n",
    "                yield np.array(batch_images), np.array(batch_labels)\n",
    "                batch_images, batch_labels = [], []\n",
    "\n",
    "        # Gerar batch incompleto se restarem imagens\n",
    "        if batch_images:\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "def lordosis_custom_generator(data_dir, lordosis_classes, target_size=(224, 224), batch_size=4, subset='training'):\n",
    "    # Mapeamento dos diretórios\n",
    "    healthy_dir = os.path.join(data_dir, 'healthy')\n",
    "    lordosis_dir = os.path.join(data_dir, 'lordosis')\n",
    "    kyphosis_and_lordosis_dir = os.path.join(data_dir, 'kyphosis_and_lordosis')\n",
    "    \n",
    "    # Carregar caminhos de imagens\n",
    "    healthy_images = [os.path.join(healthy_dir, img) for img in os.listdir(healthy_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    lordosis_images = [os.path.join(lordosis_dir, img) for img in os.listdir(lordosis_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    kyphosis_and_lordosis_images = [os.path.join(kyphosis_and_lordosis_dir, img) for img in os.listdir(kyphosis_and_lordosis_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    \n",
    "    # Associação das imagens com as classes\n",
    "    images = healthy_images + lordosis_images + kyphosis_and_lordosis_images\n",
    "    labels = [0] * len(healthy_images) + [1] * (len(lordosis_images) + len(kyphosis_and_lordosis_images))\n",
    "    labels = to_categorical(labels, num_classes=2)\n",
    "\n",
    "    # Divisão entre treinamento e validação\n",
    "    split_index = int(len(images) * 0.8)\n",
    "    if subset == 'training':\n",
    "        images, labels = images[:split_index], labels[:split_index]\n",
    "    else:\n",
    "        images, labels = images[split_index:], labels[split_index:]\n",
    "\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Embaralhar as imagens e labels\n",
    "        indices = np.arange(len(images))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in indices:\n",
    "            img = load_img(images[i], target_size=target_size, color_mode='grayscale')  # Alterado para RGB\n",
    "            img_array = img_to_array(img)\n",
    "            batch_images.append(img_array)\n",
    "            batch_labels.append(labels[i])\n",
    "\n",
    "            if len(batch_images) == batch_size:\n",
    "                yield np.array(batch_images), np.array(batch_labels)\n",
    "                batch_images, batch_labels = [], []\n",
    "\n",
    "        # Gerar batch incompleto se restarem imagens\n",
    "        if batch_images:\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd613387",
   "metadata": {},
   "outputs": [],
   "source": [
    "lordosis_classes = ['healthy', 'lordosis']\n",
    "\n",
    "lordosis_train_generator = lordosis_custom_generator(\n",
    "    data_dir = './balanced-dataset',\n",
    "    lordosis_classes = lordosis_classes,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 4,\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "lordosis_validation_generator = lordosis_custom_generator(\n",
    "    data_dir = './balanced-dataset',\n",
    "    lordosis_classes = lordosis_classes,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 4,\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "lordosis_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56e4f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "kiphosis_classes = ['healthy', 'kyphosis']\n",
    "\n",
    "kiphosis_train_generator = kyphosis_custom_generator(\n",
    "    data_dir = './balanced-dataset',\n",
    "    kiphosis_classes = kiphosis_classes,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 4,\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "kiphosis_validation_generator = kyphosis_custom_generator(\n",
    "    data_dir = './balanced-dataset',\n",
    "    kiphosis_classes = kiphosis_classes,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 4,\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "kiphosis_cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06ed8b",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a9ee0",
   "metadata": {},
   "source": [
    "Como estamos usando uma rede pre-treinada para facilitar o treinamento (a rede já identifica curvas, linhas e padrões mais simples), presisamos limpar a sessão de treinamento antes de treinar novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3845f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c465692f",
   "metadata": {},
   "source": [
    "O modelo generalista, por ter uma pre-definição de classes, ele não identifica de forma automática a quantidade de passos por época, para calcular isso, vamos utilizar o método `len()`, que retorna o tamanho de um objeto e dividir pelo tamanho do batch.\n",
    "\n",
    "Para os demais modelos, a quantidade de passos será basicamente a quantidade de imagens divido pela tamanho do batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68f8137a-a15c-43cc-b8b6-45539b2ea6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './balanced-dataset'\n",
    "batch_size = 4 \n",
    "target_size = (224, 224)\n",
    "\n",
    "def list_kyphosis_images(data_dir):\n",
    "    healthy_dir = os.path.join(data_dir, 'healthy')\n",
    "    kyphosis_dir = os.path.join(data_dir, 'kyphosis')\n",
    "    kyphosis_and_lordosis_dir = os.path.join(data_dir, 'kyphosis_and_lordosis')\n",
    "\n",
    "    healthy_images = [os.path.join(healthy_dir, img) for img in os.listdir(healthy_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    kyphosis_images = [os.path.join(kyphosis_dir, img) for img in os.listdir(kyphosis_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    kyphosis_and_lordosis_images = [os.path.join(kyphosis_and_lordosis_dir, img) for img in os.listdir(kyphosis_and_lordosis_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "\n",
    "    images = healthy_images + kyphosis_images + kyphosis_and_lordosis_images\n",
    "    labels = [0] * len(healthy_images) + [1] * (len(kyphosis_images) + len(kyphosis_and_lordosis_images))  # 0 para healthy, 1 para kyphosis\n",
    "    labels = to_categorical(labels, num_classes=2)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def list_lordosis_images(data_dir):\n",
    "    healthy_dir = os.path.join(data_dir, 'healthy')\n",
    "    lordosis_dir = os.path.join(data_dir, 'lordosis')\n",
    "    kyphosis_and_lordosis_dir = os.path.join(data_dir, 'kyphosis_and_lordosis')\n",
    "\n",
    "    healthy_images = [os.path.join(healthy_dir, img) for img in os.listdir(healthy_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    lordosis_images = [os.path.join(lordosis_dir, img) for img in os.listdir(lordosis_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "    kyphosis_and_lordosis_images = [os.path.join(kyphosis_and_lordosis_dir, img) for img in os.listdir(kyphosis_and_lordosis_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "\n",
    "    images = healthy_images + lordosis_images + kyphosis_and_lordosis_images\n",
    "    labels = [0] * len(healthy_images) + [1] * (len(lordosis_images) + len(kyphosis_and_lordosis_images))  # 0 para healthy, 1 para kyphosis\n",
    "    labels = to_categorical(labels, num_classes=2)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "kyphosis_images, kyphosis_labels = list_kyphosis_images(data_dir)\n",
    "lordosis_images, lordosis_labels = list_lordosis_images(data_dir)\n",
    "\n",
    "kyphosis_split_index = int(len(kyphosis_images) * 0.8)\n",
    "lordosis_split_index = int(len(lordosis_images) * 0.8)\n",
    "\n",
    "kyphosis_train_images, kyphosis_val_images = kyphosis_images[:kyphosis_split_index], kyphosis_images[kyphosis_split_index:]\n",
    "lordosis_train_images, lordosis_val_images = lordosis_images[:lordosis_split_index], lordosis_images[lordosis_split_index:]\n",
    "\n",
    "kyphosis_steps_per_epoch = len(kyphosis_train_images) // batch_size\n",
    "kyphosis_validation_steps = len(kyphosis_val_images) // batch_size\n",
    "lordosis_steps_per_epoch = len(lordosis_train_images) // batch_size\n",
    "lordosis_validation_steps = len(lordosis_val_images) // batch_size\n",
    "\n",
    "generalist_steps_per_epoch = len(labels) // initial_generalist_train_generator.batch_size\n",
    "generalist_validation_steps = len(labels) // initial_generalist_validation_generator.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b34fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.9159 - loss: 1.4518 - val_accuracy: 0.6087 - val_loss: 2.5917 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 946ms/step - accuracy: 0.9065 - loss: 1.4001 - val_accuracy: 0.8913 - val_loss: 1.4694 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 923ms/step - accuracy: 0.9218 - loss: 1.3334 - val_accuracy: 0.6087 - val_loss: 1.9852 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 765ms/step - accuracy: 0.9346 - loss: 1.3648 - val_accuracy: 0.9783 - val_loss: 1.3641 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 781ms/step - accuracy: 0.9684 - loss: 1.2114 - val_accuracy: 0.7826 - val_loss: 1.4922 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 805ms/step - accuracy: 0.9478 - loss: 1.1516 - val_accuracy: 0.8261 - val_loss: 1.4753 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 834ms/step - accuracy: 0.9720 - loss: 1.0966 - val_accuracy: 0.8696 - val_loss: 1.2404 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 853ms/step - accuracy: 0.8521 - loss: 1.2119 - val_accuracy: 0.8696 - val_loss: 1.2942 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 886ms/step - accuracy: 0.9217 - loss: 1.1658 - val_accuracy: 0.7826 - val_loss: 1.3604 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 911ms/step - accuracy: 0.9285 - loss: 1.0840 - val_accuracy: 0.9565 - val_loss: 1.0004 - learning_rate: 5.0000e-04\n",
      "Epoch 1/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 1s/step - accuracy: 0.8373 - loss: 1.5378 - val_accuracy: 0.6739 - val_loss: 1.8614 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 974ms/step - accuracy: 0.8500 - loss: 1.4500 - val_accuracy: 0.9348 - val_loss: 1.2613 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 954ms/step - accuracy: 0.8732 - loss: 1.3871 - val_accuracy: 0.8043 - val_loss: 1.3907 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 849ms/step - accuracy: 0.8544 - loss: 1.3059 - val_accuracy: 0.5870 - val_loss: 1.9129 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 886ms/step - accuracy: 0.9203 - loss: 1.2153 - val_accuracy: 0.5870 - val_loss: 2.0340 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 883ms/step - accuracy: 0.9326 - loss: 1.1313 - val_accuracy: 0.8043 - val_loss: 1.2757 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 882ms/step - accuracy: 0.9157 - loss: 1.1369 - val_accuracy: 1.0000 - val_loss: 0.9749 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 903ms/step - accuracy: 0.9677 - loss: 1.0135 - val_accuracy: 0.6957 - val_loss: 1.2313 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 892ms/step - accuracy: 0.9349 - loss: 0.9922 - val_accuracy: 0.9348 - val_loss: 0.9277 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 907ms/step - accuracy: 0.9410 - loss: 0.9596 - val_accuracy: 0.6304 - val_loss: 1.7781 - learning_rate: 5.0000e-04\n",
      "Epoch 1/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - accuracy: 0.6941 - loss: 2.5944 - val_accuracy: 1.0000 - val_loss: 0.8734 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.7752 - val_accuracy: 1.0000 - val_loss: 0.4296 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.3990 - val_accuracy: 1.0000 - val_loss: 0.2860 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.2610 - val_accuracy: 1.0000 - val_loss: 0.1774 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.1749 - val_accuracy: 1.0000 - val_loss: 0.1273 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.1281 - val_accuracy: 1.0000 - val_loss: 0.0955 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0960 - val_accuracy: 1.0000 - val_loss: 0.0708 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0723 - val_accuracy: 1.0000 - val_loss: 0.0551 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0563 - val_accuracy: 1.0000 - val_loss: 0.0439 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0449 - val_accuracy: 1.0000 - val_loss: 0.0351 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "lordosis_history = lordosis_cnn_model.fit(\n",
    "    lordosis_train_generator,\n",
    "    steps_per_epoch = lordosis_steps_per_epoch,\n",
    "    validation_data = lordosis_validation_generator,\n",
    "    validation_steps = lordosis_validation_steps,\n",
    "    epochs = 10,\n",
    "    callbacks = [lr_scheduler]\n",
    ")\n",
    "\n",
    "kiphosis_cnn_model.fit(\n",
    "    kiphosis_train_generator,\n",
    "    steps_per_epoch = kyphosis_steps_per_epoch,\n",
    "    validation_data = kiphosis_validation_generator,\n",
    "    validation_steps = kyphosis_validation_steps,\n",
    "    epochs = 10,\n",
    "    callbacks = [lr_scheduler]\n",
    ")\n",
    "\n",
    "generalist_history = generalist_cnn_model.fit(\n",
    "    generalist_train_generator,\n",
    "    validation_data = generalist_validation_generator,\n",
    "    steps_per_epoch = generalist_steps_per_epoch,\n",
    "    validation_steps = generalist_validation_steps,\n",
    "    epochs = 10,\n",
    "    callbacks = [lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b8cf4-89af-4aa1-bc2b-c4f160b02f93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Porque resultados sempre são diferentes:\n",
    "- Ao inicializar o modelo os pesos da rede neural são inicializados de forma aleatória, e como o processo de otimização do modelo começa a partir de diferentes pontos cada execução pode levar a resultados diferentes, por isso, cada vez que o código for executado os resultados não serão exatamente os mesmos, mas aproximados.\n",
    "- Image Augmentation: técnicas de rotação, deslocamento, zoom e etc. aplicadas na imagem para criar novas variações para realizar o treinamento do modelo, sendo também um processo aleatório.\n",
    "- Shuffling: Durante o treinamento, os dados de treino são embaralhados a cada época. Isso garante que o modelo não aprenda de forma dependente da ordem dos exemplos, mas também pode fazer com que os resultados variem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5bfd3e",
   "metadata": {},
   "source": [
    "# Validação dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a386ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 629ms/step - accuracy: 1.0000 - loss: 0.0351\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 563ms/step - accuracy: 0.9568 - loss: 0.9531\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 585ms/step - accuracy: 0.6479 - loss: 1.8741\n",
      "Validação modelo generalista - Loss: 0.0350940078496933, Acurácia: 1.0\n",
      "Validação modelo de lordose - Loss: 0.948793888092041, Acurácia: 0.97826087474823\n",
      "Validação modelo de cifose - Loss: 1.7229458093643188, Acurácia: 0.6739130616188049\n"
     ]
    }
   ],
   "source": [
    "generalist_val_loss, generalist_val_acc = generalist_cnn_model.evaluate(generalist_validation_generator, steps = generalist_validation_steps)\n",
    "lordosis_val_loss, lordosis_val_acc = lordosis_cnn_model.evaluate(lordosis_validation_generator, steps = lordosis_validation_steps)\n",
    "kiphosis_val_loss, kiphosis_val_acc = kiphosis_cnn_model.evaluate(kiphosis_validation_generator, steps = kyphosis_validation_steps)\n",
    "\n",
    "print(f\"Validação modelo generalista - Loss: {generalist_val_loss}, Acurácia: {generalist_val_acc}\")\n",
    "print(f\"Validação modelo de lordose - Loss: {lordosis_val_loss}, Acurácia: {lordosis_val_acc}\")\n",
    "print(f\"Validação modelo de cifose - Loss: {kiphosis_val_loss}, Acurácia: {kiphosis_val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2017a",
   "metadata": {},
   "source": [
    "# Função de predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae6ef124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Imagem saudável:\n",
      "Predição (modelo generalista): [[9.9966615e-01 2.9946500e-04 3.5025459e-04]]\n",
      "Classe prevista (modelo generalista): healthy\n",
      "Predição (modelo de lordose): [[0.18739401 0.812606  ]]\n",
      "Classe prevista (modelo de lordose): lordosis\n",
      "Predição (modelo de cifose): [[0.15936992 0.84063005]]\n",
      "Classe prevista (modelo de cifose): kyphosis\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\n",
      "Imagem cifose:\n",
      "Predição (modelo generalista): [[9.9966645e-01 2.9918845e-04 3.4992685e-04]]\n",
      "Classe prevista (modelo generalista): healthy\n",
      "Predição (modelo de lordose): [[0.189899 0.810101]]\n",
      "Classe prevista (modelo de lordose): lordosis\n",
      "Predição (modelo de cifose): [[0.16025211 0.8397479 ]]\n",
      "Classe prevista (modelo de cifose): kyphosis\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\n",
      "Imagem lordose:\n",
      "Predição (modelo generalista): [[9.9966991e-01 2.9606718e-04 3.4638913e-04]]\n",
      "Classe prevista (modelo generalista): healthy\n",
      "Predição (modelo de lordose): [[0.18827374 0.8117263 ]]\n",
      "Classe prevista (modelo de lordose): lordosis\n",
      "Predição (modelo de cifose): [[0.15757139 0.84242857]]\n",
      "Classe prevista (modelo de cifose): kyphosis\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\n",
      "Imagem cifose e lordose:\n",
      "Predição (modelo generalista): [[9.9966359e-01 3.0180268e-04 3.5284073e-04]]\n",
      "Classe prevista (modelo generalista): healthy\n",
      "Predição (modelo de lordose): [[0.18606631 0.8139337 ]]\n",
      "Classe prevista (modelo de lordose): lordosis\n",
      "Predição (modelo de cifose): [[0.1607426 0.8392574]]\n",
      "Classe prevista (modelo de cifose): kyphosis\n"
     ]
    }
   ],
   "source": [
    "def predict_image(image_path, model):\n",
    "    img = preprocess_image(image_path)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    return predicted_class, prediction\n",
    "    \n",
    "healthy_img = './balanced-dataset/healthy/healthy_001.jpg'\n",
    "lordosis_img = './balanced-dataset/lordosis/lordosis_001.jpg'\n",
    "kyphosis_img = './balanced-dataset/kyphosis/kyphosis_001.jpg'\n",
    "kyphosis_and_lordosis_img = './balanced-dataset/kyphosis_and_lordosis/kyphosis_and_lordosis_001.jpg'\n",
    "\n",
    "generalist_predicted_class, generalist_result = predict_image(healthy_img, generalist_cnn_model)\n",
    "lordosis_predicted_class, lordosis_result = predict_image(healthy_img, lordosis_cnn_model)\n",
    "kiphosis_predicted_class, kiphosis_result = predict_image(healthy_img, kiphosis_cnn_model)\n",
    "\n",
    "print(f'Imagem saudável:')\n",
    "print(f'Predição (modelo generalista): {generalist_result}')\n",
    "print(f'Classe prevista (modelo generalista): {generalist_classes[generalist_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de lordose): {lordosis_result}')\n",
    "print(f'Classe prevista (modelo de lordose): {lordosis_classes[lordosis_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de cifose): {kiphosis_result}')\n",
    "print(f'Classe prevista (modelo de cifose): {kiphosis_classes[kiphosis_predicted_class]}')\n",
    "\n",
    "generalist_predicted_class, generalist_result = predict_image(kyphosis_img, generalist_cnn_model)\n",
    "lordosis_predicted_class, lordosis_result = predict_image(kyphosis_img, lordosis_cnn_model)\n",
    "kiphosis_predicted_class, kiphosis_result = predict_image(kyphosis_img, kiphosis_cnn_model)\n",
    "\n",
    "print(f'\\nImagem cifose:')\n",
    "print(f'Predição (modelo generalista): {generalist_result}')\n",
    "print(f'Classe prevista (modelo generalista): {generalist_classes[generalist_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de lordose): {lordosis_result}')\n",
    "print(f'Classe prevista (modelo de lordose): {lordosis_classes[lordosis_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de cifose): {kiphosis_result}')\n",
    "print(f'Classe prevista (modelo de cifose): {kiphosis_classes[kiphosis_predicted_class]}')\n",
    "\n",
    "generalist_predicted_class, generalist_result = predict_image(lordosis_img, generalist_cnn_model)\n",
    "lordosis_predicted_class, lordosis_result = predict_image(lordosis_img, lordosis_cnn_model)\n",
    "kiphosis_predicted_class, kiphosis_result = predict_image(lordosis_img, kiphosis_cnn_model)\n",
    "\n",
    "print(f'\\nImagem lordose:')\n",
    "print(f'Predição (modelo generalista): {generalist_result}')\n",
    "print(f'Classe prevista (modelo generalista): {generalist_classes[generalist_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de lordose): {lordosis_result}')\n",
    "print(f'Classe prevista (modelo de lordose): {lordosis_classes[lordosis_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de cifose): {kiphosis_result}')\n",
    "print(f'Classe prevista (modelo de cifose): {kiphosis_classes[kiphosis_predicted_class]}')\n",
    "\n",
    "generalist_predicted_class, generalist_result = predict_image(kyphosis_and_lordosis_img, generalist_cnn_model)\n",
    "lordosis_predicted_class, lordosis_result = predict_image(kyphosis_and_lordosis_img, lordosis_cnn_model)\n",
    "kiphosis_predicted_class, kiphosis_result = predict_image(kyphosis_and_lordosis_img, kiphosis_cnn_model)\n",
    "\n",
    "print(f'\\nImagem cifose e lordose:')\n",
    "print(f'Predição (modelo generalista): {generalist_result}')\n",
    "print(f'Classe prevista (modelo generalista): {generalist_classes[generalist_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de lordose): {lordosis_result}')\n",
    "print(f'Classe prevista (modelo de lordose): {lordosis_classes[lordosis_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de cifose): {kiphosis_result}')\n",
    "print(f'Classe prevista (modelo de cifose): {kiphosis_classes[kiphosis_predicted_class]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
