{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eb3e39-098e-483c-9bb2-d5127a6bf8d7",
   "metadata": {},
   "source": [
    "# Instalação e importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ec8da3-3c51-4afc-99cd-949940be1d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless numpy tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1506d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 02:27:13.784336: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-09 02:27:13.799735: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-09 02:27:13.950737: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-09 02:27:14.081814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 02:27:14.198561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 02:27:14.233387: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 02:27:14.510511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 02:27:16.098609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d1588-4b4f-48d2-a603-dc813b703855",
   "metadata": {},
   "source": [
    "**Atenção:** O aviso acima apenas informa que não foram encontrados drivers cuda e por conta disso será usada a CPU da máquina ao invés da GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28962c8a",
   "metadata": {},
   "source": [
    "# Coleta e processamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b1ed72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    img_normalized = img_resized / 255.0\n",
    "    return img_normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6807d",
   "metadata": {},
   "source": [
    "# Balanceamento do DataSet\n",
    "Para evitar maior peso em um determinado tipo de dado, foi feito um balanceamento do dataset.\n",
    "\n",
    "Para isso, foi utilizado o método de reamostragem chamado de undersampling, no qual foi selecionado aleatoriamente os dados de uma classe e duplicado até que a quantidade de dados de cada classe se iguale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ad1f84-059e-439a-9177-d6eaa951629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset balanceado criado em: ./balanced-dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = './dataset'\n",
    "balanced_dataset_dir = './balanced-dataset'\n",
    "\n",
    "classes = ['healthy', 'kyphosis', 'lordosis']\n",
    "\n",
    "os.makedirs(balanced_dataset_dir, exist_ok=True)\n",
    "\n",
    "image_counts = {}\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(dataset_dir, cls)\n",
    "    image_counts[cls] = len(os.listdir(cls_path))\n",
    "\n",
    "max_images = max(image_counts.values())\n",
    "new_image_counts = {}\n",
    "\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(balanced_dataset_dir, cls), exist_ok=True)\n",
    "    \n",
    "    images = os.listdir(os.path.join(dataset_dir, cls))\n",
    "    \n",
    "    for index, image in enumerate(images):\n",
    "        new_name = f\"{cls}_{index + 1:03d}.jpg\"\n",
    "        shutil.copy(os.path.join(dataset_dir, cls, image), \n",
    "                    os.path.join(balanced_dataset_dir, cls, new_name))\n",
    "    \n",
    "    new_image_counts[cls] = len(images)\n",
    "\n",
    "    current_count = new_image_counts[cls]\n",
    "    while current_count < max_images:\n",
    "        image_to_copy = random.choice(images)\n",
    "        \n",
    "        new_name = f\"{cls}_{current_count + 1:03d}.jpg\"\n",
    "        shutil.copy(os.path.join(dataset_dir, cls, image_to_copy),\n",
    "                    os.path.join(balanced_dataset_dir, cls, new_name))\n",
    "        \n",
    "        current_count += 1\n",
    "\n",
    "print(\"Dataset balanceado criado em:\", balanced_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e21c3d",
   "metadata": {},
   "source": [
    "### Criando CNN (rede neural convolucional)\n",
    "Essa função cria um modelo CNN que passa por algumas camadas convolucionais para extrair características das imagens, nas quais foram utilizados as funções:\n",
    "- ReLU \n",
    "\t- Função de ativação que permite que a rede neural aprenda padrões complexos\n",
    "- Pooling\n",
    "\t- Operação de amostragem usada em redes convolucionais para reduzir as dimensões e/ou tamanho da imagem ou das saídas das camadas convolucionais, porém, preservando suas características mais importantes\n",
    "- Normalização de Batch\n",
    "\t- Normalização de dados de entrada para que a rede neural possa aprender mais rapidamente\n",
    "- Softmax\n",
    "\t- Função de ativação que converte um vetor de valores reais em uma distribuição de probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5430f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3_class_cnn(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Converte a imagem grayscale para 3 canais\n",
    "    x = layers.Conv2D(3, (1, 1), padding=\"same\")(inputs)\n",
    "    \n",
    "    # Base do modelo ResNet50 com pesos da ImageNet\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Passa pela base do modelo\n",
    "    x = base_model(x)\n",
    "    \n",
    "    # Adicionando mais convoluções e pooling\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Verifique o tamanho da saída após as convoluções e pooling\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)  # Criar o modelo corretamente\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_2_class_cnn(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Converte a imagem grayscale para 3 canais\n",
    "    x = layers.Conv2D(3, (1, 1), padding=\"same\")(inputs)\n",
    "    \n",
    "    # Base do modelo ResNet50 com pesos da ImageNet\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Passa pela base do modelo\n",
    "    x = base_model(x)\n",
    "    \n",
    "    # Adicionando mais convoluções e pooling\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Verifique o tamanho da saída após as convoluções e pooling\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)  # Criar o modelo corretamente\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define o shape de entrada com 1 canal (grayscale)\n",
    "input_shape = (224, 224, 1)\n",
    "\n",
    "# Criação dos modelos\n",
    "generalist_cnn_model = create_3_class_cnn(input_shape)\n",
    "lordosis_cnn_model = create_2_class_cnn(input_shape)\n",
    "kiphosis_cnn_model = create_2_class_cnn(input_shape)\n",
    "\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885e959",
   "metadata": {},
   "source": [
    "# Pré-processamento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf630b2-1805-4776-b3d6-32cb745ef3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    brightness_range = [0.8, 1.2],\n",
    "    validation_split = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3832254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 images belonging to 3 classes.\n",
      "Found 15 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "generalist_classes = ['healthy', 'kyphosis', 'lordosis']\n",
    "\n",
    "generalist_train_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "generalist_validation_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "generalist_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd613387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "lordosis_classes = ['healthy', 'lordosis']\n",
    "\n",
    "lordosis_train_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    classes = lordosis_classes\n",
    ")\n",
    "\n",
    "lordosis_validation_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation',\n",
    "    classes = lordosis_classes\n",
    ")\n",
    "\n",
    "lordosis_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56e4f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "kiphosis_classes = ['healthy', 'kyphosis']\n",
    "\n",
    "kiphosis_train_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    classes = kiphosis_classes\n",
    ")\n",
    "\n",
    "kiphosis_validation_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation',\n",
    "    classes = kiphosis_classes\n",
    ")\n",
    "\n",
    "kiphosis_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06ed8b",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b34fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3043 - loss: 4.7112 - val_accuracy: 0.2667 - val_loss: 4.1938 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.3930 - loss: 4.1567 - val_accuracy: 0.4667 - val_loss: 3.8413 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.4428 - loss: 4.1630 - val_accuracy: 0.4667 - val_loss: 3.6349 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.5900 - loss: 3.6679 - val_accuracy: 0.2000 - val_loss: 3.8179 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.5023 - loss: 3.9634 - val_accuracy: 0.4667 - val_loss: 3.6675 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.4279 - loss: 3.9559 - val_accuracy: 0.3333 - val_loss: 3.5652 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.7073 - loss: 3.2250 - val_accuracy: 0.6000 - val_loss: 3.4552 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.6312 - loss: 3.4130 - val_accuracy: 0.4000 - val_loss: 3.5276 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.6426 - loss: 3.3115 - val_accuracy: 0.3333 - val_loss: 3.6293 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.4117 - loss: 3.3936 - val_accuracy: 0.4667 - val_loss: 3.4659 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.6030 - loss: 3.4657 - val_accuracy: 0.6667 - val_loss: 3.4076 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.5315 - loss: 3.3782 - val_accuracy: 0.6000 - val_loss: 3.3606 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.5771 - loss: 3.1375 - val_accuracy: 0.6000 - val_loss: 3.3413 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.4759 - loss: 3.6057 - val_accuracy: 0.5333 - val_loss: 3.4052 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.5426 - loss: 3.1924 - val_accuracy: 0.5333 - val_loss: 3.3239 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.5714 - loss: 3.4443 - val_accuracy: 0.7333 - val_loss: 3.1855 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.6887 - loss: 3.1638 - val_accuracy: 0.2667 - val_loss: 3.3195 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.4986 - loss: 3.3079 - val_accuracy: 0.6667 - val_loss: 3.0856 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.5779 - loss: 3.3819 - val_accuracy: 0.6000 - val_loss: 3.1035 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.6019 - loss: 3.1566 - val_accuracy: 0.6667 - val_loss: 3.2345 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.7070 - loss: 2.9977 - val_accuracy: 0.6000 - val_loss: 3.1744 - learning_rate: 5.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.5210 - loss: 3.4007 - val_accuracy: 0.4000 - val_loss: 3.2606 - learning_rate: 2.5000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.6268 - loss: 2.9753 - val_accuracy: 0.7333 - val_loss: 3.0984 - learning_rate: 2.5000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.7022 - loss: 2.8789 - val_accuracy: 0.4667 - val_loss: 3.1716 - learning_rate: 2.5000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.6318 - loss: 3.1358 - val_accuracy: 0.5333 - val_loss: 3.2388 - learning_rate: 1.2500e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.7311 - loss: 2.9865 - val_accuracy: 0.6000 - val_loss: 3.2014 - learning_rate: 1.2500e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.5961 - loss: 3.2420 - val_accuracy: 0.6667 - val_loss: 2.9126 - learning_rate: 1.2500e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.5889 - loss: 3.1142 - val_accuracy: 0.6667 - val_loss: 2.9633 - learning_rate: 1.2500e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.6700 - loss: 3.0183 - val_accuracy: 0.4000 - val_loss: 3.3548 - learning_rate: 1.2500e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.6791 - loss: 2.8702 - val_accuracy: 0.5333 - val_loss: 3.1303 - learning_rate: 1.2500e-04\n",
      "Epoch 1/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.6214 - loss: 3.7404 - val_accuracy: 0.7000 - val_loss: 3.6319 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.6411 - loss: 3.7670 - val_accuracy: 0.5000 - val_loss: 3.4229 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.6821 - loss: 3.3017 - val_accuracy: 0.5000 - val_loss: 3.4002 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.4429 - loss: 3.9871 - val_accuracy: 0.5000 - val_loss: 3.4526 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.7054 - loss: 3.2584 - val_accuracy: 0.6000 - val_loss: 3.1571 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.6470 - loss: 3.1835 - val_accuracy: 0.3000 - val_loss: 3.2546 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.6250 - loss: 3.3766 - val_accuracy: 0.3000 - val_loss: 3.1625 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5440 - loss: 3.3720 - val_accuracy: 0.6000 - val_loss: 3.0388 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5518 - loss: 3.4318 - val_accuracy: 0.4000 - val_loss: 3.1313 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.7720 - loss: 2.9965 - val_accuracy: 0.6000 - val_loss: 2.9235 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5268 - loss: 3.2281 - val_accuracy: 0.5000 - val_loss: 2.9446 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5905 - loss: 3.3538 - val_accuracy: 0.5000 - val_loss: 2.9077 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5607 - loss: 3.4825 - val_accuracy: 0.5000 - val_loss: 3.0414 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.6863 - loss: 2.8479 - val_accuracy: 0.5000 - val_loss: 3.3405 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.1887 - loss: 3.4461 - val_accuracy: 0.8000 - val_loss: 2.8609 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.5256 - loss: 3.1389 - val_accuracy: 0.5000 - val_loss: 2.9822 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.6542 - loss: 2.9469 - val_accuracy: 0.9000 - val_loss: 2.7413 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5339 - loss: 2.9956 - val_accuracy: 0.5000 - val_loss: 2.9154 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5589 - loss: 3.1784 - val_accuracy: 0.5000 - val_loss: 2.9381 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.8946 - loss: 2.6010 - val_accuracy: 0.6000 - val_loss: 2.7443 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.6107 - loss: 2.9787 - val_accuracy: 0.8000 - val_loss: 2.7629 - learning_rate: 5.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.4929 - loss: 3.0438 - val_accuracy: 0.8000 - val_loss: 2.6709 - learning_rate: 5.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.5417 - loss: 3.0219 - val_accuracy: 0.7000 - val_loss: 2.7063 - learning_rate: 5.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6952 - loss: 3.0437 - val_accuracy: 0.9000 - val_loss: 2.5774 - learning_rate: 5.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.6810 - loss: 2.8923 - val_accuracy: 0.8000 - val_loss: 2.6739 - learning_rate: 5.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.5548 - loss: 2.8539 - val_accuracy: 0.8000 - val_loss: 2.6284 - learning_rate: 5.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7054 - loss: 2.8645 - val_accuracy: 0.7000 - val_loss: 2.6916 - learning_rate: 5.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6208 - loss: 2.7577 - val_accuracy: 0.7000 - val_loss: 2.7728 - learning_rate: 2.5000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6940 - loss: 2.7297 - val_accuracy: 0.8000 - val_loss: 2.7227 - learning_rate: 2.5000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.6048 - loss: 2.7637 - val_accuracy: 0.8000 - val_loss: 2.6143 - learning_rate: 2.5000e-04\n",
      "Epoch 1/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.4161 - loss: 4.1588 - val_accuracy: 0.5000 - val_loss: 3.6732 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7887 - loss: 3.4813 - val_accuracy: 0.5000 - val_loss: 3.5918 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5786 - loss: 3.4152 - val_accuracy: 0.5000 - val_loss: 3.3840 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.9131 - loss: 3.0024 - val_accuracy: 0.5000 - val_loss: 3.2819 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.6208 - loss: 3.3880 - val_accuracy: 0.5000 - val_loss: 3.1974 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7607 - loss: 3.0218 - val_accuracy: 0.5000 - val_loss: 3.2074 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7619 - loss: 3.1068 - val_accuracy: 0.5000 - val_loss: 3.1824 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.6655 - loss: 3.2379 - val_accuracy: 0.7000 - val_loss: 2.9837 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.5661 - loss: 3.0048 - val_accuracy: 0.6000 - val_loss: 2.8882 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.9024 - loss: 2.7959 - val_accuracy: 0.7000 - val_loss: 2.9300 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6851 - loss: 3.0393 - val_accuracy: 0.9000 - val_loss: 2.7535 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.8488 - loss: 2.7050 - val_accuracy: 0.9000 - val_loss: 2.7697 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7756 - loss: 2.6810 - val_accuracy: 0.8000 - val_loss: 2.6739 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8946 - loss: 2.8011 - val_accuracy: 0.7000 - val_loss: 2.7396 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9042 - loss: 2.4718 - val_accuracy: 0.9000 - val_loss: 2.5526 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.8036 - loss: 2.6185 - val_accuracy: 1.0000 - val_loss: 2.5540 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.9143 - loss: 2.4328 - val_accuracy: 1.0000 - val_loss: 2.5424 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.8506 - loss: 2.8360 - val_accuracy: 0.9000 - val_loss: 2.4811 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7970 - loss: 2.6234 - val_accuracy: 0.9000 - val_loss: 2.4807 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7500 - loss: 2.7606 - val_accuracy: 1.0000 - val_loss: 2.4076 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.8268 - loss: 2.3834 - val_accuracy: 1.0000 - val_loss: 2.4550 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.8131 - loss: 2.4680 - val_accuracy: 0.8000 - val_loss: 2.5883 - learning_rate: 0.0010\n",
      "Epoch 23/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9220 - loss: 2.3854 - val_accuracy: 0.9000 - val_loss: 2.3899 - learning_rate: 0.0010\n",
      "Epoch 24/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7661 - loss: 2.8088 - val_accuracy: 0.8000 - val_loss: 2.5006 - learning_rate: 0.0010\n",
      "Epoch 25/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.9292 - loss: 2.2108 - val_accuracy: 0.8000 - val_loss: 2.5124 - learning_rate: 0.0010\n",
      "Epoch 26/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7131 - loss: 2.8242 - val_accuracy: 0.6000 - val_loss: 2.5251 - learning_rate: 0.0010\n",
      "Epoch 27/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8226 - loss: 2.4187 - val_accuracy: 0.7000 - val_loss: 2.5707 - learning_rate: 5.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6554 - loss: 2.8339 - val_accuracy: 0.6000 - val_loss: 2.6016 - learning_rate: 5.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.8595 - loss: 2.3909 - val_accuracy: 0.6000 - val_loss: 2.5016 - learning_rate: 5.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7500 - loss: 2.4834 - val_accuracy: 0.5000 - val_loss: 2.6213 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "generalist_history = generalist_cnn_model.fit(\n",
    "    generalist_train_generator,\n",
    "    validation_data = generalist_validation_generator,\n",
    "    epochs = 30,\n",
    "    callbacks = [lr_scheduler]\n",
    ")\n",
    "\n",
    "lordosis_history = lordosis_cnn_model.fit(\n",
    "    lordosis_train_generator,\n",
    "    validation_data = lordosis_validation_generator,\n",
    "    epochs = 30,\n",
    "    callbacks = [lr_scheduler]\n",
    ")\n",
    "\n",
    "kiphosis_history = kiphosis_cnn_model.fit(\n",
    "    kiphosis_train_generator,\n",
    "    validation_data = kiphosis_validation_generator,\n",
    "    epochs = 30,\n",
    "    callbacks = [lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b8cf4-89af-4aa1-bc2b-c4f160b02f93",
   "metadata": {},
   "source": [
    "## Porque resultados sempre são diferentes:\n",
    "- Ao inicializar o modelo os pesos da rede neural são inicializados de forma aleatória, e como o processo de otimização do modelo começa a partir de diferentes pontos cada execução pode levar a resultados diferentes, por isso, cada vez que o código for executado os resultados não serão exatamente os mesmos, mas aproximados.\n",
    "- Image Augmentation: técnicas de rotação, deslocamento, zoom e etc. aplicadas na imagem para criar novas variações para realizar o treinamento do modelo, sendo também um processo aleatório.\n",
    "- Shuffling: Durante o treinamento, os dados de treino são embaralhados a cada época. Isso garante que o modelo não aprenda de forma dependente da ordem dos exemplos, mas também pode fazer com que os resultados variem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5bfd3e",
   "metadata": {},
   "source": [
    "# Validação dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a386ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 926ms/step - accuracy: 0.5567 - loss: 2.9815\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 958ms/step - accuracy: 0.8188 - loss: 2.6659\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 697ms/step - accuracy: 0.6313 - loss: 2.4899\n",
      "Validação modelo generalista - Loss: 3.0154881477355957, Acurácia: 0.6000000238418579\n",
      "Validação modelo de lordose - Loss: 2.738570213317871, Acurácia: 0.699999988079071\n",
      "Validação modelo de cifose - Loss: 2.4555375576019287, Acurácia: 0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "generalist_val_loss, generalist_val_acc = generalist_cnn_model.evaluate(generalist_validation_generator)\n",
    "lordosis_val_loss, lordosis_val_acc = lordosis_cnn_model.evaluate(lordosis_validation_generator)\n",
    "kiphosis_val_loss, kiphosis_val_acc = kiphosis_cnn_model.evaluate(kiphosis_validation_generator)\n",
    "\n",
    "print(f\"Validação modelo generalista - Loss: {generalist_val_loss}, Acurácia: {generalist_val_acc}\")\n",
    "print(f\"Validação modelo de lordose - Loss: {lordosis_val_loss}, Acurácia: {lordosis_val_acc}\")\n",
    "print(f\"Validação modelo de cifose - Loss: {kiphosis_val_loss}, Acurácia: {kiphosis_val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2017a",
   "metadata": {},
   "source": [
    "# Função de predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae6ef124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Predição (modelo generalista): [[0.03289848 0.08524974 0.8818518 ]]\n",
      "Classe prevista (modelo generalista): lordosis\n",
      "Predição (modelo de lordose): [[0.41304484 0.58695513]]\n",
      "Classe prevista (modelo de lordose): lordosis\n",
      "Predição (modelo de cifose): [[0.6733316  0.32666835]]\n",
      "Classe prevista (modelo de cifose): healthy\n"
     ]
    }
   ],
   "source": [
    "def predict_image(image_path, model):\n",
    "    img = preprocess_image(image_path)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    return predicted_class, prediction\n",
    "\n",
    "generalist_predicted_class, generalist_result = predict_image('./test-data/001.jpg', generalist_cnn_model)\n",
    "lordosis_predicted_class, lordosis_result = predict_image('./test-data/001.jpg', lordosis_cnn_model)\n",
    "kiphosis_predicted_class, kiphosis_result = predict_image('./test-data/001.jpg', kiphosis_cnn_model)\n",
    "\n",
    "print(f'Predição (modelo generalista): {generalist_result}')\n",
    "print(f'Classe prevista (modelo generalista): {generalist_classes[generalist_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de lordose): {lordosis_result}')\n",
    "print(f'Classe prevista (modelo de lordose): {lordosis_classes[lordosis_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de cifose): {kiphosis_result}')\n",
    "print(f'Classe prevista (modelo de cifose): {kiphosis_classes[kiphosis_predicted_class]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
